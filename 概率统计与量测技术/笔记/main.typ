#import "../../note_zh.typ": *
#show: conf.with(
  title: "概率统计分析与量测技术笔记",
  author: "杨哲涵"
)
= 集合以及事件
#def([$sigma$-field])[
  $sigma$-field, or $sigma$-algebra, is a collection of subsets of a set $S$ that is closed under countable unions, countable intersections, and complements.
]
#thm()[
  Let $cal(O)$ be one of the 8 set operations, let ${cal(C)_t,t in T}$ be an indexed family of subsets such that for each $t$, $cal(C)_t$ is closed under $cal(O)$. Then
  $ cal(C)=sect_(t in T) "is closed under" cal(O) $
]
#coll()[
  The intersection of a $sigma$ fields is a $sigma$ field.
]
#def([minimal $sigma$-field])[
  Let $cal(C)$ be a collection of subsets of $Omega$. The $sigma$-field generated by $cal(C)$, denoted by $sigma(cal(C))$, is a $sigma$-field satisfying the following conditions:
  - $cal(C) subset sigma(cal(C))$
  - If $cal(B)'$ is some other $sigma$-field containing $cal(C)$, then $sigma(cal(C)) subset cal(B)'$
]
#thm()[
  Given a class $cal(C)$ of subsets of $Omega$, there exists a unique smallest $sigma$-field containing $cal(C)$.
]
#def("Borel Sets")[
  Suppose $Omega=RR$ and let $ cal(C)={(a,b]-infinity<=a<=b<=+infinity} $ Then $cal(B)(RR):=sigma(cal(C)) $ the Borel subsets of $RR$.

  There are many equivalent ways to define the Borel sets.
]
= 随机事件与概率
#def("事件域")[
  设$S$为样本空间,$cal(F)$为$S$的某些子集组成的集合类.如果$cal(F)$满足下列条件,称$cal(F)$为$S$的一个事件域.
  - $S in cal(F)$
  - $A in cal(F) -> overline(A) in cal(F)$
  - $A_n in cal(F),n=1,2,dots -> union.big_(n=1)^K A_n in cal(F)$
]
#def("概率")[
  1933 年柯尔莫哥洛夫(Kolmogorov)基于集合论给出.

  设$S$为样本空间,$cal(F)$是由$S$的某些子集组成的一个事件域.如果对任意事件$A in cal(F)$,定义在$cal(F)$上的一个实值函数$P(A)$满足
  / 非负性公理: $A in cal(F) -> P(A)>=0$
  / 正则性公理(规范性公理): $P(S)=1$
  / 可列可加性公理: 若$A_1,A_2,dots,A_n,dots$互斥,则$P(union.big_(i=1)^infinity A_i)=sum_(i=1)^(infinity)P(A_i)$
  那么称$P$为概率.$(S,cal(F),P)$为概率空间.
]
== 几何概型
古典概型的局限是样本空间离散,基本事件数有限.

当随机试验的样本空间是某连续区域$S$,并且任意一点落在度量(长度,面积,体积)相同的子区域是等可能的,则事件$A$的概率可定义为
$ P(A)=m(A)/m(S) $
几何概型基于现代的 "测度" 的概念,
== 贝叶斯概率
贝叶斯概率的样本空间中的样本点为一系列*假设(hypotheses)*
= 连续型随机变量
#def("连续型随机变量")[
  设$X$是随机变量,若存在一个非负可积函数$f(x)$,使得$ F(x)=integral_(-infinity)^x f(t) dd(t),-infinity<x<+infinity $
  则称$X$是连续性随机变量,函数$F(x)$是它的分布函数,函数$f(x)$是它的概率密度函数,简称概率密度或密度函数.
]
分布函数的有用之处在于,把连续型随机变量与离散型随机变量统一了起来.
#def("指数分布")[
  设$X$是一个连续型随机变量,若它的概率密度函数为
  $ f(x)=cases(lambda e^(-lambda x) &"if" x>0,0 &"if" x<=0) $
  则称$X$服从参数为$lambda$的指数分布,记为$X~"Exp"(lambda)$.
]
常作为各种 "寿命" 分布的近似
- 不稳定粒子的寿命
- 无线电元件的寿命
#thm("指数分布无记忆")[
  若$X~"Exp"(lambda)$,则$P(X>s+t|X>s)=P(X>t)$. 指数分布是 "永远年轻" 的分布.
]
#proof[
  $ P(X>s+t|X>s)=&P(X>s+t)/P(X>s)\ =&(1-P(X<=s+t))/(1-P(X<=s))\ =&(1-F(s+t))/(1-F(s))\ =&e^(-lambda(s+t))/e^(-lambda s)\ =&e^(-lambda t)=P(X>t) $
]
指数分布作为 "寿命" 分布的近似,并不是例如人的寿命的实际分布.

几何分布作为也是无记忆的,可以认为是离散型随机变量中的无记忆分布.

离散型随机变量是右连续的.
#def("柯西分布")[
又称Breit-Wigner分布,概率密度函数为
$ f(x;x_0,gamma)=1/pi gamma/((x-x_0)^2+gamma^2) $
$x_0=0,gamma=1$的特例被称为标准柯西分布.
]

#thm("随机变量概率密度的复合")[
设随机变量$X$具有概率密度$f_X (x),-infinity<x<infinity$.设函数$g(x)$处处可导且恒有$g'(x)>0$或$g'(x)<0$.则随机变量$Y=g(X)$是连续型随机变量,其概率密度为
$ f_Y (y)=cases(f_X (h(y)) abs(h'(y)) &"if" alpha < y< beta,0 &"elsewhere") $
其中$h(y)$是$g(x)$的反函数$ alpha=min(g(-infinity),g(infinity)),beta=max(g(-infinity),g(infinity)) $
]
= 二维随机变量
注意$X$,$Y$来自同一个样本空间,意味着$X$,$Y$可以不独立.
= 期望
#thm("柯西-施瓦茨不等式")[
$ E(X^2)E(Y^2)>=E(X Y)^2 $
]
= 大数定律
#thm("Chebyshev不等式")[
设随机变量$X$有数学期望$E(X)=mu$和方差$"Var"(X)=sigma^2$,则:$ forall epsilon>0, P(abs(x-mu)>=epsilon)<=sigma^2/epsilon^2 $
]
#def("Khinchin")[辛钦大数定律是弱大数定律,设$X_1,X_2,dots$是相互独立,服从同一分布的随机变量序列,且具有数学期望$E(X_k)=mu,k=1,2,dots$,则$forall epsilon >0$,有
$ lim_(n->infinity) P(abs(1/n sum_(k=1)^n X_k -mu)<epsilon)=1 $
]
#coll("Bernoulli")[伯努利大数定律是辛钦大数定律的重要推论,设$f_A$是$n$次独立重复试验中事件$A$发生的次数,$p$是事件$A$在每次试验中发生的概率,则$forall epsilon >0$,有
$ lim_(n->infinity) P(abs(f_A/n-p)<epsilon)=1 $
或
$ lim_(n->infinity) P(abs(f_A/n-p)>=epsilon)=0 $
]

= 极限定理
极限定理是概率论的核心内容之一.
/ 中心极限定理: 什么条件下$sum_(i=1)^n X_i$的分布收敛于正态分布
  - 独立不同分布
    - 李雅普诺夫
  - 独立同分布
    - 林德伯格-列维
    - 棣莫弗-拉普拉斯
#thm("Lindberg Levi")[
假设随机变量序列$X_1,X_2,dots$独立同分布,且数学期望和方差存在$ E(X_k)=mu, "Var"(X_k)=sigma^2>0 $
则随机变量之和$X=:sum_(k=1)^n X_k$的标准化变量
$ Y_n=(X-n mu)/(sqrt(n)sigma) $
的分布函数$F_n (x)$对于任意实数$x$满足
$ lim_(n->infinity) F_n (x)=integral_(-infinity)^x 1/sqrt(2pi) e^(-t^2/2) dd(t)=Phi(x) $
$n$足够大时, $X$近似服从$N(n mu,n sigma^2)$
]
#proof[
对于$ Y_n=(sum_(k=1)^n X_k-n mu)/(sqrt(n)sigma)=sum_(i=1)^n (X_i-mu)/(sqrt(n)sigma) $
$ => phi_(Y_n) (t)=(phi_(X_i)-mu (t/(sqrt(n)sigma)))^n $
]
#coll("De Moivre-Laplace")[
这是Lindberg-Levi中心极限定理的二项分布特例.

设$Y_n~b(n,p), 0<p<1, n=1,2,dots$,则$forall x in RR$,有
$ lim_(n->infinity) P((Y_n-n p)/sqrt(n p(1-p))<=x)=1/sqrt(2pi)integral_(-infinity)^x e^(-t^2/2) dd(t)=Phi(x) $
]
如何理解中心极限定理?

卷积操作是高斯分布的不动点.
#thm("Lyapunov")[
李雅普诺夫定理说,设$X_1,X_2,dots$是独立随机变量序列,且数学期望和方差存在$ E(X_k)=mu_k, "Var"(X_k)=sigma_k^2>0, 1<=k<=n $
记$B_n^2=sum_(k=1)^n sigma_k^2$.
如果存在$delta>0$,使得Lyapunov条件
$ lim_(n->infinity) 1/B_n^(2+delta) sum_(k=1)^n E(abs(X_k-mu_k))=0 $
成立,则随机变量之和$X=:sum_(k=1)^n X_k$的标准化变量$ Y_n=(sum_(i=1)^n X_k-sum_(k=1)^n mu_k)/B_n $
的分布函数$F_n (x)$对于任意$x$满足$ lim_(n->infinity) F_n (x)=Phi(x) $
]
#exmp[
对于柯西分布,由于其方差不存在,所以中心极限定理不成立.
]
#thm("马尔科夫中心极限定理")[
对于相互不独立的随机变量,即设$X_1,X_2,dots$是随机变量序列,且满足无后效性,即$P(X_j|X_(j-1),X_(j-2),dots)=P(X_j|X_(j-1))$
且满足可逆性和可达性条件,使它成为一个马尔科夫链,那么
$ mu=E(X_1) $
$ sigma^2="Var"(X_1)+2sum_(k=1)^infinity "Cov"(X_1,X_(1+k))<+infinity $
]
= 蒙特卡罗方法
#def("第一类舍选法")[
希望采样分布$X=x in [a,b], X~f(x)$且$L=max{f(x)|a<=x<=b}$
+ 对$[a,b]cprod[0,1]$区域内均匀分布的二维随机变量$(U,V)$抽样
+ 保留曲线$v=f(u)/L$下方的点,取其横坐标$ {u_i|v_i<=f(u_i)/L,i=1,2,dots,n}={x_1,x_2,dots,x_n} $
]
#thm("舍选法效率的期望")[
$ E=1/((b-a)L) $
]
#proof[
舍选法可以采样到希望的分布,是因为
$ P(X<=x)&=P(U<=x|V<=f(U)/L)\ &=P(U<=x,V<=f(U)/L)/P(V<=f(U)/L)\ &=(integral_a^x dd(u) integral_0^(f(u)/L) dd(v) g(u,v))/(integral_a^b dd(u) integral_0^(f(u)/L) dd(v) g(u,v))\ &=integral_a^x dd(u) f(u) $
其中$g(u,v)=1/(b-a)$.此外,效率的期望为
$ E&=P(V<=f(U)/L)\ &=integral_a^b dd(u)integral_0^(f(u)/L)dd(v)g(u,v)\ &=1/((b-a)L) $
]
= 分布族
#def("指数分布族")[
一族的分布,它们的概率密度函数或分布律记为$f(x|va(theta))$,如果有如下的形式,则称为$s$维指数分布族
$ f(x|va(theta))=exp(sum_(i=1)^s eta_i (va(theta)) T_i (x)-B(va(theta))) h(x) $
]
#def("指数分布族标准形式")[
把 $eta_i (va(theta))$当作自变量反解$theta$,那么形式有进一步化简.
$ f(x|va(eta))=exp(sum_(i=1)^s eta_i T_i (x)-A(va(eta))) h(x) $
其中$eta_i=eta_i (va(theta)),A(eta(va(theta)))=B(va(theta))$
]
#thm[指数分布族的期望和方差为$ E(X)=A'(eta) $$ "Var"(X)=A''(eta) $]
#exmp("二项分布属于指数分布族")[
$ P(k|p,n)&=binom(n,k)p^k(1-p)^(n-k)\ &=h(k) exp(T(k) eta(p)-B(p)) $
其中$ h(k)=binom(n,k),T(k)=k,eta(p)=ln(p/(1-p)),B(p)=-n ln(1-p) $
]
#exmp("正态分布属于指数分布族")[
$ f(x|mu,sigma^2)&=1/sqrt(2pi sigma^2) exp(-(x-mu)^2/(2sigma^2)) $
]
= 统计学概论
#def("箱线图")[
箱线图是一种用作显示一组数据分散情况的统计图表,显示了一组数据的最大值,最小值,中位数,上四分位数和下四分位数.

这五个分位数分别称为$"Min",Q_1,M,Q_3,"Max"$.

常用于比较不同组别的数据分布情况.
]
= 统计量
#def("样本均值")[
设$(X_1,X_2,dots,X_n)$是来自总体$X$的样本
$ overline(X)=1/n sum_(i=1)^n X_i $
]
#def("样本方差")[
设$(X_1,X_2,dots,X_n)$是来自总体$X$的样本
$ S^2=1/(n-1) sum_(i=1)^n (X_i-overline(X))^2 $
]
#def("样本标准差")[
设$(X_1,X_2,dots,X_n)$是来自总体$X$的样本
$ S=sqrt(S^2) $
]
#def([样本$k$阶原点矩])[
设$(X_1,X_2,dots,X_n)$是来自总体$X$的样本
$ A_k=1/n sum_(i=1)^n X_i^k $
]
#def([样本$k$阶中心矩])[
设$(X_1,X_2,dots,X_n)$是来自总体$X$的样本
$ B_k=1/n sum_(i=1)^n (X_i-overline(X))^k $
]
#thm("偏差平方和最小")[
数据观察值与样本均值的偏差平方和最小,即在形如$sum_(i=1)^n (X_i -c)^2$的函数中,$sum_(i=1)^n (X_i -overline(X))^2$
]
#thm("样本均值方差和矩的联系")[
- $E(overline(X))=mu$
- $"Var"(overline(X))=sigma^2\/n$
- $E(S^2)=sigma^2$
]
#proof[
由于$(X_1,X_2,dots,X_n)$独立同分布于$X$,从而有
$ "Var"(overline(X))&=sum_(i=1)^n "Var"(1/n X_i)\ &= sum_(i=1)^n 1/n^2 "Var"(X)\ &= sigma^2/n $
$ E(B_2)&=E(1/n sum_(i=1)^n X_i^2)-E(overline(X)^2)\ &=1/n (n("Var"(X)+E^2(X)))-("Var"(overline(X))+E^2(overline(X)))\ &="Var"(X)+(E^2(X))/n -1/n "Var"(X)-1/n E^2(X)\ &=(n-1)/n "Var"(X) $
由于$E(B_2)=(n-1)/n E(S^2)$,从而得证.
]
#thm("中心极限定理")[
设$X_1,X_2,dots,X_n$是来自某个总体的样本,$overline(X)$是样本均值.
- 若总体分布为$N(mu,sigma^2)$,则$overline(X)~ N(mu,sigma^2/n)$
- 若总体分布未知或不是正态分布,但$E(X)=mu,"Var"(X)=sigma^2$存在,则$n$较大时,$overline(X)$的渐进分布为$N(mu,sigma^2/n)$
]
如何理解$chi^2$分布在自由度越大的时候越接近正态分布?
#quote[具有可加性的分布,因为中心极限定理,当自由度越大时,分布的形状越接近正态分布.]
#def([$F$分布])[
设随机变量$X ~ chi^2(n),Y ~ chi^2(m)$,且$X,Y$相互独立.
称$ F=(X/n)\/(Y/m) $为服从第一自由度为$n$,第二自由度为$m$的$F$分布.
$ f_F (x)=(Gamma((m+n)/2)binom(n,m)^(n/2))/(Gamma(m/2)Gamma(n/2))x^(n/2-1)(1+n/m x)^(-(m+n)/2) $
]
#thm([$F$分布的特性])[
- 若$F ~ F(n,m)$,则$1/F ~ F(m,n)$
- $F_(1-alpha) (n,m)=1/(F_alpha (m,n))$
]
#def([$T$分布])[
设随机变量$X ~ N(0,1),Y ~ chi^2(n)$,且$X,Y$相互独立.
称$ T=X/sqrt(Y\/n) $为服从自由度为$n$的$T$分布.
$ f_T (t)=t f_T^2 (t^2)=Gamma((n+1)/2)/Gamma(n/2)sqrt(n pi)(1+t^2/n)^(-(n+1)/2), t in (-infinity,+infinity) $
]
#thm("两个正态总体的比较")[
设总体$X~N(mu,sigma^2)$,样本为$(X_1,X_2,dots,X_n)$,又设总体$X' ~ N(mu',sigma'^2)$,样本为$(X'_1,X'_2,dots,X'_n)$,则
$ S^2/sigma^2 S'^2/sigma'^2 ~ F(n-1,n'-1) $
]
= 统计推断
== 前言
/ 参数估计与非参数估计:
  / 参数估计: 当推断的对象是有限个,例如高斯总体的期望,方差
  / 非参数估计: 当推断的对象是无限个,例如未知分布总体的期望,方差,分布

/ 参数估计类型:
  / 点估计: 估计未知参数的值
  / 区间估计: 估计未知参数的取值范围,并使此范围包含未知参数真值的概率为给定的值
#exmp[
$X~N(mu, sigma^2)$,若$mu, sigma$未知,通过构建统计量,给出它们的估计值(点估计)或取值范围(区间估计)就是参数估计的内容.
]
== 参数估计方法
#def("点估计")[
用一个数值作为未知参数的估计值称为点估计.

设总体$X$的分布函数的形式已知,$theta$是待估参数,$(X_1,X_2,dots,X_n)$为总体的一个样本.

点估计构造一个恰当的统计量$hat(theta) (X_1, X_2,dots,X_n)$,用它的观察值$hat(theta) (x_1, x_2,dots,x_n)$作为待估参数$theta$的近似.
]
#def("矩估计")[
用样本$k$阶矩作为总体$k$阶矩的估计量,建立含待估参数的方程,从而解出待估参数.

设随机变量$X~f(x;theta_1,theta_2,dots,theta_k)$,其中$theta_1,theta_2,dots,theta_k$为待估参数.假设样本总体的前$k$阶矩存在
$ E(X^r)=mu_r (theta_1,theta_2,dots,theta_k), 1<=r<=k $
假设$(X_1,X_2,dots,X_n)$为来自$X$的一个样本,$r$阶样本矩$A_r=1/n sum_(i=1)^n X_i^r$.

$A_r$及其函数依概率收敛于相应的总体矩.因此可以
- 用样本矩作为相应的总体矩的估计量
- 用样本矩的函数作为相应的总体矩函数的估计量

总体的前$k$阶矩构成联立方程组,含$k$个未知参数.一般情况下可以解出这$k$个参数$theta_1,theta_2,dots,theta_k$.
$ cases(mu_1=mu_1(theta_1,theta_2,dots,theta_k),
  mu_2=mu_2(theta_1,theta_2,dots,theta_k),dots.v,
  mu_k=mu_k (theta_1,theta_2,dots,theta_k)
) =>
cases(theta_1=theta_1(mu_1,mu_2,dots,mu_k),
  theta_2=theta_2(mu_1,mu_2,dots,mu_k),dots.v,
  theta_k=theta_k (mu_1,mu_2,dots,mu_k)
) $
用样本矩$A_r$代替总体矩$mu_r$阶得到待估参数的估计量,称为*矩估计量*.
$ hat(theta)_i=theta_i (A_1,A_2,dots,A_k), i=1,2,dots,k $
矩估计量的观测值称为*矩估计值*.
]
#exmp("最大似然法的引入")[
设总体$X$服从0-1分布,且$P(X=1)=p$,用最大似然法求$p$的估计值
设$x_1,x_2,dots,x_n$为总体的样本的估计值,则得到该样本值的概率为
$ P&(X_1=x_1,dots,X_n=x_n)\ &=product_(i=1)^n P(X_i=x_i)\ &=p^(sum_(i=1)^n x_i) (1-p)^(1-sum_(i=1)^n x_i) =: L(p) $
对于不同的$p$,有$L(p)$不同,取$p$使这个事件发生的概率最大
$ hat(p)=arg max L(p) = arg max log L(p) $
由于 $ dv(,p)log L=(sum_(i=1)^n x_i )/p- (n-sum_(i=1)^n x_i)/(1-p)\ => hat(p)=overline(x) $
]
#def("似然函数")[
  - 设$X$为离散型随机变量,分布律为$P(X=x)=p(x,theta)$,则似然函数定义为
  $ L(va(x),theta)=product_(i=1)^n p(x_i,theta) $
  - 设$X$是连续型随机变量,取$f(X,theta)$为$X$的密度函数,则似然函数定义为
  $ L(va(x),theta)=product_(i=1)^n f(x_i,theta) $
]
#def("最大似然估计,maximum likelihood estimation")[
  $ hat(va(theta)) (va(x),theta)=arg max L(va(x);va(theta)) $
称为最大似然估计估计值,称统计量$hat(va(theta)) (X_1,X_2,dots,X_n)$为参数$va(theta)$的最大参数估计量.
]
#exmp("均匀分布的矩估计和最大似然估计")[
设$X~U(a,b)$,有$X_1,X_2,dots,X_n$是$X$的一个样本.求$a,b$的矩估计和最大似然估计.

使用矩估计有
$ mu_1=E(X)=(a+b)/2\ mu_2=E(X^2)="Var"(X)+E^2(X)=(b-a)^2/12+(a+b)^2/4 $
解得
$ va(a)&=A_1-sqrt(3(A_2-A_1^2))=A_1-sqrt(3 B_2)\ va(b)&=A_1+sqrt(3(A_2-A_1^2))=A_1+sqrt(3 B_2) $
使用最大似然估计有
$ L(a,b)=cases(1/(b-a)^2 &"where" a<x_i<b,0 &"elsewhere") $
当$a=min x_i,b=max x_i$时,$L(a,b)$最大,所以
$ hat(a)=min X_i, hat(b)=max X_i $
*这两种估计的结果不同.*
]
== 估计的评价
#thm("期望方差")[
  设$hat(theta)_n (X_1,dots,X_n)$是$theta$的一个估计量.若
  $ lim_(n->infinity) E(hat(theta)_n)=theta, lim_(n->infinity) "Var"(hat(theta)_n)=0 $
  则$hat(theta)_n$是$theta$的相合估计量的相合估计量.
] <evaluation_1>

#thm("均方误差")[
  $ "MSE" (hat(theta)_n)=E(hat(theta)_n-theta)^2="Var"(hat(theta)_n)-E^2(hat(theta)_n-theta) $
  等价于 @evaluation_1.
]
